"""Whitestone Group Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e9N6402YodpaaPagutSYN3EMsQbQBVQc
"""

# Importing libraries
import pandas as pd

# Reading csv files into DataFrames
outreach = pd.read_csv('C:/Users/vaishnavidixit/Downloads/Group Project Data-20241104/Group Project Data/1. Outreach summary.csv')
open = pd.read_csv('C:/Users/vaishnavidixit/Downloads/Group Project Data-20241104/Group Project Data/2. Opens Email.csv')
sent = pd.read_csv('C:/Users/vaishnavidixit/Downloads/Group Project Data-20241104/Group Project Data/3. Sent Emails.csv')
master = pd.read_csv('C:/Users/vaishnavidixit/Downloads/Group Project Data-20241104/Group Project Data/4. company master data.csv')

# Display the first few rows of the outreach DataFrame
print(outreach.head())

# Dropping unnecessary columns from outreach
columns_to_remove = ['Bounces', 'Bounce Rate', 'Unsubscribes', 'Unsubscribe Rate']
outreach.drop(columns=columns_to_remove, inplace=True)

# Adding a new column to track email opens
open['Opened At'] = 1
print(open.head())

# Removing rows where 'Email status' is 'Unsubscribed'
open.drop(open[open['Email status'] == 'Unsubscribed'].index, inplace=True)
print(open.head())

# Checking for duplicates in the open DataFrame
print(open.duplicated().sum())

# Displaying the sent DataFrame
print(sent.head())

# Removing rows where 'Email status' is 'Unsubscribed' in the sent DataFrame
sent.drop(sent[sent['Email status'] == 'Unsubscribed'].index, inplace=True)

# Displaying the master DataFrame
print(master.head())

# Checking for duplicates in sent DataFrame
print(sent.duplicated().sum())

# Checking for missing values in all DataFrames
print(outreach.isnull().sum())
print(open.isnull().sum())

# Dropping irrelevant columns from the open DataFrame
columns_to_remove = ['Email status', 'Email permission status', 'Confirmed Opt-Out Date',
                     'Confirmed Opt-Out Reason', 'Website', 'Title', 'Country - Home']
open.drop(columns=columns_to_remove, inplace=True)
print(open.head())

# Checking for missing values in sent DataFrame
print(sent.isnull().sum())

# Dropping irrelevant columns from sent DataFrame
sent.drop(columns=columns_to_remove, inplace=True)
print(sent.head())

# Checking for missing values in master DataFrame
print(master.isnull().sum())

# Displaying information about the open and sent DataFrames
print(open.info())
print(sent.info())

# Checking and handling duplicates in sent DataFrame
print(sent.duplicated().sum())
duplicates = sent.duplicated(keep=False)  # Identifies all duplicate rows
print(sent[duplicates])  # Displays duplicate rows
sent = sent.drop_duplicates(subset=['Campaign Name', 'Company', 'City - Home',
                                    'State/Province - Home', 'Created At',
                                    'Updated At', 'Sent At'], keep='first')

# Removing duplicates in the open DataFrame using similar logic
open = open.drop_duplicates(subset=['Campaign Name', 'Company', 'City - Home',
                                    'State/Province - Home', 'Created At',
                                    'Updated At', 'Opened At'], keep='first')

# Saving cleaned sent DataFrame to CSV
sent.to_csv('sent.csv', index=False)

# Verifying duplicates removal in sent DataFrame
sent.duplicated(subset=['Campaign Name', 'Company', 'City - Home',
                        'State/Province - Home', 'Created At', 'Updated At',
                        'Sent At']).sum()

# Displaying cleaned open DataFrame
print(open.info())

# Merging sent and open DataFrames on common columns
Open_Sent = pd.merge(sent, open, on=['Campaign Name', 'Company'], how='left')

# Dropping duplicate columns after merge
Open_Sent.drop(columns=['City - Home_y', 'State/Province - Home_y',
                        'Created At_y', 'Updated At_y'], inplace=True)

# Renaming columns for clarity
Open_Sent.rename(columns={'City - Home_x': 'City - Home',
                          'State/Province - Home_x': 'State/Province - Home',
                          'Created At_x': 'Created At', 'Updated At_x': 'Updated At'}, inplace=True)

# Filling NaN values in 'Opened At' column with 0
Open_Sent['Opened At'] = Open_Sent['Opened At'].fillna(0)

# Removing duplicate rows post-merge
Open_Sent.drop_duplicates(subset=['Company', 'Campaign Name', 'City - Home',
                                  'State/Province - Home', 'Created At',
                                  'Updated At', 'Sent At', 'Opened At'], inplace=True)

# Saving the cleaned Open_Sent DataFrame to CSV
Open_Sent.to_csv('Open_Sent.csv', index=False)

# Merging cleaned outreach with Open_Sent
Email_outreach = Open_Sent.merge(outreach, on='Campaign Name', how='left')

# Dropping unnecessary columns post-merge
Email_outreach.drop(columns=['Time Sent'], inplace=True)

# Converting 'Sent At' to datetime and creating separate date and time columns
Email_outreach['Time Sent'] = pd.to_datetime(Email_outreach['Sent At'], format='%m/%d/%Y %I:%M%p')
Email_outreach['Date Sent'] = Email_outreach['Time Sent'].dt.date
Email_outreach['Time_Sent'] = Email_outreach['Time Sent'].dt.time

# Dropping irrelevant columns
Email_outreach.drop(columns=['Sends', 'Opens', 'Clicks', 'Sent At',
                             'Created At', 'Updated At'], inplace=True)

# Renaming a column for consistency
Email_outreach = Email_outreach.rename(columns={'Opened At': 'Opened'})

# Mapping state names to abbreviations and creating a new column
state_mapping = {
    'Minnesota': 'MN', 'MN': 'MN', 'Wisconsin': 'WI', 'WI': 'WI',
    'Illinois': 'IL', 'IL': 'IL', 'Ohio': 'OH', 'OH': 'OH',
    'Indiana': 'IN', 'IN': 'IN', 'Michigan': 'MI', 'MI': 'MI',
    'Ga': 'GA', 'GA': 'GA', 'SC': 'SC'
}
Email_outreach['State'] = Email_outreach['State/Province - Home'].map(state_mapping)

# Dropping unnecessary columns
Email_outreach.drop(columns=['State/Province - Home', 'Open Rate',
                             'Click Rate', 'Time Sent'], inplace=True)

# Saving the final cleaned DataFrame to CSV
Email_outreach.to_csv('Email_outreach.csv', index=False)

# Printing the cleaned DataFrame
print(Email_outreach.head(50))

# Checking for duplicate rows in the final DataFrame
duplicate_rows = Email_outreach.duplicated(subset=["Campaign Name", "Company", "Date Sent"], keep=False)
print(Email_outreach[duplicate_rows])

# Displaying information about the final DataFrame
print(Email_outreach.info())

# Get the unique values in the 'State/Province - Home' column
Email_outreach['State/Province - Home'].unique()

# Create a new column 'State' with default values set to None
Email_outreach['State'] = None

# Define a mapping dictionary to standardize state names and abbreviations
state_mapping = {
    'Minnesota': 'MN',
    'MN': 'MN',
    'Wisconsin': 'WI',
    'WI': 'WI',
    'Illinois': 'IL',
    'IL': 'IL',
    'Ohio': 'OH',
    'OH': 'OH',
    'Indiana': 'IN',
    'IN': 'IN',
    'Michigan': 'MI',
    'MI': 'MI',
    'Ga': 'GA',
    'GA': 'GA',
    'SC': 'SC'
}

# Use the mapping dictionary to update the 'State' column based on 'State/Province - Home'
Email_outreach['State'] = Email_outreach['State/Province - Home'].map(state_mapping)

# Print the unique values in the updated 'State' column
print(Email_outreach['State'].unique())

# Remove the original 'State/Province - Home' column as it is no longer needed
Email_outreach.drop(columns=['State/Province - Home'], inplace=True)

# Drop unnecessary columns related to email metrics
Email_outreach.drop(columns=['Open Rate', 'Click Rate', 'Time Sent'], inplace=True)

# Save the updated DataFrame to a CSV file
Email_outreach.to_csv('Email_outreach.csv', index=False)

# Print the first 50 rows of the updated DataFrame
print(Email_outreach.head(50))

# Identify duplicate rows based on specific columns
duplicate_rows = Email_outreach.duplicated(subset=["Campaign Name", "Company", "Date Sent"], keep=False)

# Print the duplicate rows
print(Email_outreach[duplicate_rows])

# Print a summary of the DataFrame
print(Email_outreach.info())

# Define a function to keep the row with the least number of null values in duplicated groups
def keep_row_with_least_nulls(df, group_columns):
    # Count the number of null values in each row
    df['null_count'] = df.isnull().sum(axis=1)

    # Group by the specified columns and retain rows with the fewest null values
    df_unique = df.loc[df.groupby(group_columns)['null_count'].idxmin()]

    # Drop the temporary 'null_count' column
    df_unique = df_unique.drop(columns='null_count')

    return df_unique

# Apply the function to deduplicate rows based on 'Company', 'Campaign Name', and 'Time_Sent'
group_columns = ['Company', 'Campaign Name', 'Time_Sent']
Email_outreach = keep_row_with_least_nulls(Email_outreach, group_columns)

# Remove remaining duplicates while keeping the first occurrence
Email_outreach.drop_duplicates(subset=["Campaign Name", "Company", 'Time_Sent'], keep='first', inplace=True)

# Print the deduplicated DataFrame
print(Email_outreach)

# Print the count of null values in each column
print(Email_outreach.isnull().sum())

# Print a summary of another DataFrame named 'master'
print(master.info())

# Print the count of null values in the 'master' DataFrame
print(master.isnull().sum())

# Normalize function to clean and standardize company names
def normalize_name(name):
    """Normalize company names by converting to lowercase and removing unwanted characters."""
    name = str(name).lower()  # Convert to lowercase
    name = re.sub(r'\s*&\s*', '&', name)  # Remove spaces around "&"
    name = re.sub(r'[^\w&]', '', name)  # Remove non-alphanumeric characters except "&"
    return name

# Sets to keep track of matched indices for Email_outreach and master DataFrames
matched_email_outreach = set()
matched_master = set()

# List of columns in 'master' to match against
columns = ['company_name', 'company_name_stevens', 'company_name_whitestone',
           'company_name_thomasnet', 'company_name_solar']

# Iterate through each row in Email_outreach to find matches in master
for idx, email_row in Email_outreach.iterrows():
    if idx in matched_email_outreach:
        continue  # Skip if this row is already matched
   
    email_outreach_company = normalize_name(email_row['Company'])  # Normalize company name
    email_outreach_first_word = email_outreach_company.split()[0]  # Extract the first word

    found_match = False  # Flag to indicate if a match is found

    # Check each column in 'master' for matches
    for col in columns:
        for master_idx, master_row in master.iterrows():
            if master_idx in matched_master:
                continue  # Skip if this row is already matched

            master_company_name = normalize_name(master_row[col])  # Normalize master company name
            if email_outreach_first_word in master_company_name:  # Match based on the first word
                master.at[master_idx, 'Company'] = email_row['Company']  # Update 'Company' in master
                matched_email_outreach.add(idx)  # Mark Email_outreach row as matched
                matched_master.add(master_idx)  # Mark master row as matched
                found_match = True
                break  # Stop further checks in this column
        if found_match:
            break  # Stop further column checks if a match is found

# Print the updated 'master' DataFrame
print(master)

# Print the count of null values in 'master'
print(master.isnull().sum())

# Calculate the mode of city-related columns in 'master'
master['City - Home'] = master[['city_stevens', 'city_thomasnet', 'city_whitestone']].mode(axis=1)[0]

# Calculate the mode of state-related columns in 'master'
master['State'] = master[['state_stevens', 'state_thomasnet', 'state_whitestone']].mode(axis=1)[0]

# Drop unused city and state columns
master.drop(columns=['city_stevens', 'city_thomasnet', 'city_whitestone', 'state_stevens', 'state_thomasnet', 'state_whitestone'], inplace=True)

# Drop unused company-related columns
master.drop(columns=['company_name', 'company_name_solar', 'company_name_stevens', 'company_name_whitestone', 'company_name_thomasnet'], inplace=True)

# Drop columns related to company details that are no longer needed
master.drop(columns=['company_age_whitestone', 'contact_title_stevens', 'contact_title_whitestone', 'founded_year_thomasnet', 'major_products,_capabilities_stevens', 'major_products,_capabilities_whitestone', 'description_thomasnet'], inplace=True)

# Print unique values in 'companytype_thomasnet'
master['companytype_thomasnet'].unique()

# Calculate the mode of revenue-related columns in 'master'
master['Revenue'] = master[['company_revenue_($m)_stevens', 'company_revenue_($m)_thomasnet', 'company_revenue_($m)_whitestone', 'revenue_range_whitestone']].mode(axis=1)[0]

# Calculate the mode of headcount-related columns in 'master'
master['Headcount'] = master[['headcount_range_whitestone', 'headcount_thomasnet', 'headcount_stevens', 'headcount_whitestone']].mode(axis=1)[0]

# Calculate the mode of zipcode-related columns in 'master'
master['Zipcode'] = master[['zip_code_stevens', 'zip_code_whitestone']].mode(axis=1)[0]

# Drop unused revenue, headcount, and zipcode columns
master.drop(columns=['company_revenue_($m)_stevens', 'company_revenue_($m)_thomasnet', 'company_revenue_($m)_whitestone', 'revenue_range_whitestone'], inplace=True)
master.drop(columns=['headcount_range_whitestone', 'headcount_thomasnet', 'headcount_stevens', 'headcount_whitestone'], inplace=True)
master.drop(columns=['zip_code_stevens', 'zip_code_whitestone'], inplace=True)

# Print the count of null values in 'master'
print(master.isnull().sum())

# Save the updated 'master' DataFrame to a CSV file
master.to_csv('master_she_is.csv', index=False)

# Print unique values in 'companytype_thomasnet'
print(master['companytype_thomasnet'].unique())

# Identify duplicate rows in 'master' based on 'Company'
duplicate_rows = master.duplicated(subset=["Company"], keep=False)

# Print the duplicate rows
print(master[duplicate_rows])

# Merge 'Email_outreach' and 'master' on 'Company' using an inner join
merged = Email_outreach.merge(master[['Company', 'State', 'City - Home', "companytype_thomasnet", "Revenue", "Headcount", "Zipcode"]],
                              on='Company',
                              how='inner',
                              suffixes=('', '_master'))

# Overwrite 'State' and 'City - Home' in 'merged' with values from 'master' if null in Email_outreach
merged['State'] = merged['State'].combine_first(merged['State_master'])
merged['City - Home'] = merged['City - Home'].combine_first(merged['City - Home_master'])

# Drop temporary '_master' columns from 'merged'
merged = merged.drop(columns=['State_master', 'City - Home_master'])

# Save the merged DataFrame to a CSV file
merged.to_csv('merged1.csv', index=False)

# Print a summary of the 'merged' DataFrame
print(merged.info())

# Training the model with Random Forest
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, ConfusionMatrixDisplay, accuracy_score, roc_auc_score, roc_curve
import seaborn as sb
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Load the cleaned dataset
file_path = 'C:/Users/dipen/OneDrive/Desktop/Adelphi University/MS BUSINESS ANALYTICS/Applied Machine Learning/Group Project Data/merged1 (1).csv'
Final = pd.read_csv(file_path)

# Convert percentage strings to numerical values
if Final['Mobile Open Rate'].dtype == 'object':
    Final['Mobile Open Rate'] = Final['Mobile Open Rate'].str.rstrip('%').astype(float) / 100
if Final['Desktop Open Rate'].dtype == 'object':
    Final['Desktop Open Rate'] = Final['Desktop Open Rate'].str.rstrip('%').astype(float) / 100

# Feature Engineering
# Convert time to categories: morning, afternoon, night
Final['Time_Sent'] = pd.to_datetime(Final['Time_Sent'], format='%H:%M:%S').dt.hour

def categorize_time(hour):
    if 6 <= hour < 12:
        return 'Morning'
    elif 12 <= hour < 18:
        return 'Afternoon'
    else:
        return 'Night'

Final['Time_Category'] = Final['Time_Sent'].apply(categorize_time)

# Convert date to weekday/weekend
Final['Date Sent'] = pd.to_datetime(Final['Date Sent'])
Final['Day_Type'] = Final['Date Sent'].dt.dayofweek.apply(lambda x: 'Weekend' if x >= 5 else 'Weekday')

# Add political party affiliation by state (simplified example)
party_affiliation = {
    'CA': 'Democrat', 'NY': 'Democrat', 'IL': 'Democrat',
    'TX': 'Republican', 'FL': 'Republican', 'OH': 'Republican'
}
Final['Political_Party'] = Final['State'].map(party_affiliation).fillna('Independent')

# Drop irrelevant or high-missing-value columns
data_cleaned = Final.drop(columns=['Campaign Name', 'Company', 'City - Home', 'Zipcode', 'Date Sent', 'Time_Sent'])

# Handle missing values
data_cleaned = data_cleaned.dropna()

# Encode categorical variables
data_cleaned = pd.get_dummies(data_cleaned, drop_first=True)

# Correlation matrix
correlation_matrix = data_cleaned.corr()
print("Correlation Matrix:")
print(correlation_matrix['Opened'].sort_values(ascending=False))

# Select top four features correlated with 'Opened'
top_features = ['Opened', 'State_MN', 'Desktop Open Rate', 'Revenue_$25 - 49.9 Mil', 'companytype_thomasnet_Service Company, Custom Manufacturer']
corr_top_features = correlation_matrix.loc[top_features, top_features]

# Create the heatmap
plt.figure(figsize=(10, 6))
sb.heatmap(corr_top_features, annot=True, cmap='coolwarm', linewidths=0.5, vmin=-1, vmax=1)
plt.title("Heatmap of Top Features Correlated with 'Opened'")
plt.show()

# Select independent variables based on correlation
correlated_features = correlation_matrix['Opened'].drop('Opened').abs().sort_values(ascending=False)
selected_features = correlated_features[correlated_features > 0.1].index.tolist()

# Prepare X (independent variables) and y (dependent variable)
X = data_cleaned[selected_features]
y = data_cleaned['Opened']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)


print("-----------Random Forest Model-----------------")

# Train the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions
y_pred = rf_model.predict(X_test)
y_pred_prob = rf_model.predict_proba(X_test)[:, 1]

# Generate classification report
print("Classification Report:")
print(classification_report(y_test, y_pred, zero_division=0))

# Plot confusion matrix
ConfusionMatrixDisplay.from_estimator(rf_model, X_test, y_test, cmap='Blues', values_format='d')
plt.title("Confusion Matrix")
plt.show()

# Calculate and print additional statistics
accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_prob)

print(f"Accuracy: {accuracy}")
print(f"ROC-AUC Score: {roc_auc}")

# Visualize ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid()
plt.show()

# Visualize feature importance
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf_model.feature_importances_
})
feature_importance = feature_importance.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
sb.barplot(x='Importance', y='Feature', data=feature_importance, palette='viridis', legend=False)
plt.title("Feature Importance")
plt.tight_layout()
plt.show()




print("-----------Gradient Boosting Machine(GBM)-----------------")

# Training the model with Gradient Boosting Machines (GBM)
import pandas as pd
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, ConfusionMatrixDisplay, roc_auc_score, roc_curve, accuracy_score
import seaborn as sb
import matplotlib.pyplot as plt

# Assuming data_cleaned and selected_features are already defined

# Prepare X (independent variables) and y (dependent variable)
X = data_cleaned[selected_features]
y = data_cleaned['Opened']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train the GBM model
gbm_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)
gbm_model.fit(X_train, y_train)

# Make predictions
y_pred = gbm_model.predict(X_test)
y_pred_prob = gbm_model.predict_proba(X_test)[:, 1]

# Generate classification report
print("Classification Report:")
print(classification_report(y_test, y_pred, zero_division=0))

# Plot confusion matrix
ConfusionMatrixDisplay.from_estimator(gbm_model, X_test, y_test, cmap='Blues', values_format='d')
plt.title("Confusion Matrix")
plt.show()

# Calculate and print additional statistics
accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_prob)

print(f"Accuracy: {accuracy}")
print(f"ROC-AUC Score: {roc_auc}")

# Visualize ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid()
plt.show()

# Visualize feature importance
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': gbm_model.feature_importances_
})
feature_importance = feature_importance.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
sb.barplot(x='Importance', y='Feature', data=feature_importance, palette='viridis', legend=False)
plt.title("Feature Importance")
plt.tight_layout()
plt.show()



print("---------------------------------SVM----------------------------------------")
#Training the model with SVM
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report, ConfusionMatrixDisplay, roc_auc_score, roc_curve, accuracy_score
import seaborn as sb
import matplotlib.pyplot as plt

# Prepare X (independent variables) and y (dependent variable)
X = data_cleaned[selected_features]
y = data_cleaned['Opened']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train the SVM model
svm_model = SVC(probability=True, kernel='linear', random_state=42)
svm_model.fit(X_train, y_train)

# Make predictions
y_pred = svm_model.predict(X_test)
y_pred_prob = svm_model.predict_proba(X_test)[:, 1]

# Calculate and print additional statistics
accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_prob)

# Generate classification report
print("Classification Report:")
print(classification_report(y_test, y_pred, zero_division=0))

# Plot confusion matrix
ConfusionMatrixDisplay.from_estimator(svm_model, X_test, y_test, cmap='Blues', values_format='d')
plt.title("Confusion Matrix")
plt.show()

print(f"Accuracy: {accuracy}")
print(f"ROC-AUC Score: {roc_auc}")

# Visualize ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid()
plt.show()

# Compute squared error
squared_errors = (y_test - y_pred) ** 2

# Calculate Mean Squared Error (MSE)
mse = squared_errors.mean()
print(f"Mean Squared Error: {mse}")

# Calculate the sum of squared errors
sum_squared_errors = squared_errors.sum()
print(f"Sum of Squared Errors: {sum_squared_errors}")



print("------------------------------Decision tree---------------------------------")
#Training the model with Decision Tree Regression
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, ConfusionMatrixDisplay, roc_auc_score, roc_curve, accuracy_score
import seaborn as sb
import matplotlib.pyplot as plt


# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train the Decision Tree model
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)

# Make predictions
y_pred = dt_model.predict(X_test)
y_pred_prob = dt_model.predict_proba(X_test)[:, 1]

# Generate classification report
print("Classification Report:")
print(classification_report(y_test, y_pred, zero_division=0))

# Plot confusion matrix
ConfusionMatrixDisplay.from_estimator(dt_model, X_test, y_test, cmap='Blues', values_format='d')
plt.title("Confusion Matrix")
plt.show()

# Calculate and print additional statistics
accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_prob)

print(f"Accuracy: {accuracy}")
print(f"ROC-AUC Score: {roc_auc}")

# Visualize ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.grid()
plt.show()

# Visualize feature importance using coefficients
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': dt_model.feature_importances_
})
feature_importance = feature_importance.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
sb.barplot(x='Importance', y='Feature', data=feature_importance, palette='viridis', legend=False)
plt.title("Feature Importance")
plt.tight_layout()
plt.show()

# Compute squared error
squared_errors = (y_test - y_pred) ** 2

# Calculate Mean Squared Error (MSE)
mse = squared_errors.mean()
print(f"Mean Squared Error: {mse}")

# Calculate the sum of squared errors
sum_squared_errors = squared_errors.sum()
print(f"Sum of Squared Errors: {sum_squared_errors}")
